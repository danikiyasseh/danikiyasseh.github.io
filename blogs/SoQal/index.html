<!DOCTYPE HTML>
<!--
	Spectral by Pixelarity
	pixelarity.com @pixelarity
	License: pixelarity.com/license
-->
<html>
	<style type="text/css">
	    .col-3 {
		width: 33%;
		float: left;
	    	}
	</style>
	
	<head>
		<link rel="icon" href="">
		<title>Dani Kiyasseh | Blog - SoQal </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                               tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                               });
        </script>
        <script type="text/javascript"
  			src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-97381584-1', 'auto');
		  ga('send', 'pageview');

		</script>

		<style>
		* {
		  box-sizing: border-box;
		}

		.column {
		  float: left;
		  width: 50%;
		  padding: 5px;
		}

		/* Clearfix (clear floats) */
		.row::after {
		  content: "";
		  clear: both;
		  display: table;
		}
		</style>
		
		

	</head>
  
  
	<body>

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="../../index.html">Dani Kiyasseh</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
									<a href="#menu" class="menuToggle"><span>Menu</span></a>
									<div id="menu">
										<ul>
											<li><a href="../../index.html">Home</a></li>
											<li><a href="../../index.html#aboutme">About Me</a></li>
											<li><a href="../../index.html#experience">Experience</a></li>
											<li>&mdash;</li>
											<li><a href="../../publications.html">Publications</a></li>
											<li><a href="../index.html">Blog</a></li>
											<li><a href="../../datasets.html">Datasets</a></li>											
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

	<!-- Main -->
	<article id="main">
		<section class="wrapper style5">

			<div class="inner">
			<h2>
			<center>
				SoQal: Selective Oracle Questioning for Consistency Based Active Learning of Cardiac Signals 
			</center>
			</h2>

			<p>
			<center>
				Sunday, May 22nd, 2022
			</center>
			</p>
			</div>

			<p>
			<center>
			Dani Kiyasseh
			</center>
			</p>
			
			<div class="container">				
			    <div class="row">
				<center>
				<div class="col"><center><img src="./images/caltech_logo.jpg" style="width:10%"></center></div>
				<div class="col"><center><img src="./images/oxford_logo.png" style="width:5%"></center></div>
				</center>
			    </div>
			</div>
			<hr>
		
		<div class="inner">
		<p>
		The healthcare industry now generates troves of data from distinct modalities on a daily basis. Such data modalities range from the micro of genomics, which capture information at the sub-cellular level, 
    to the macro of physiological signals and medical images, which capture information at the organ level. Extracting clinically-meaningful insight from such data can be achieved via deep learning algorithms, which
    are notoriously data-hungry (read: require abundant labelled data). 
		</p>
			
		<p>
		Oftentimes, however, the rate with which this data are generated far exceeds the rate with which 
    they can be realistically annotated by medical professionals. For example, in low-resource clinical settings, medical professionals may lack the necessary expertise or simply be unavailable to provide annotations. 
    Conversely, in high-resource clinical settings, medical professionals are inundated by the sheer number of requests for annotations, exacerbating the increasingly-observed phenomenon of physician-burnout. As such, 
    clinical settings are now commonly characterized by the presence of limited labelled data and abundant unlabelled data. One way to deal with this scenario is through active learning, as outlined next.
		</p>
                
    <center>
	<img src="./images/labelled_and_unlabelled_data.png" style="width:60%">
    </center>
        
    <h4>
    Introduction to Active Learning
    </h4>
    <p>
    An active learning framework allows neural networks to achieve strong performance while simultaneously exploiting a labelled and unlabelled set of data and minimizing the annotation burden placed on an oracle 
    (e.g., a physician). All such frameworks typically iterate over four main steps: 1) <i><b> train </b></i> - a neural network is trained on some existing labelled data, 2) <i><b> acquire </b></i> - the same network is tasked with 
    acquiring unlabelled data points, 3) <i><b> label </b></i> - an oracle (e.g., physician) is tasked with labelling such acquired data points, and 4) <i><b> augment </b></i> - the neural network is trained on the existing and 
    newly-labelled data points. Whereas previous work in the literature commonly focuses on either step 2 (acquisition) or step 3 (annotation), we modify both. 
    </p>
                
	<center>
	    <img src="./images/active_learning.gif" style="width:60%">
	</center>
                
    <h4>
    Limitations of Existing Methods to Active Learning
    </h4>
    <h5>
    How does previous work acquire unlabelled datapoints?
    </h5>
    <p>
    To acquire datapoints from a pool of unlabelled data, active learning methods rank these datapoints from the most informative to the least informative. 
    Given datapoints from two distinct classes (see below), it is believed that datapoints which are closer to the decision boundary are more informative than those
    farther away from the boundary. Previous work quantifies this proximity to the decision boundary via Monte Carlo Dropout (MCD), which first perturbs the parameters  
    of a neural network to generate distinct decision boundaries, and identifies unlabelled datapoints whose network classification differs across the parameter perturbations. 
    We show, however, that if such parameter perturbations are misspecified (e.g. by being too small in magnitude), this approach can fail to acquire otherwise informative datapoints, thus
    hindering the learning process.
    </p>
				
  	<center>
	    <img src="./images/MCD.gif" style="width:60%">
	</center>
				
    <h5>
    How does previous work annotate newly-acquired unlabelled datapoints?
    </h5>    
    <p>
    Previous work typically assumes the presence of an ideal oracle (e.g., physician) who is capable of providing correct annotations for any and all of the acquired unlabelled datapoints. This assumption, however, is unlikely to hold 
    within healthcare where physicians can either lack expertise for a particular task (generating incorrect annotations) or simply be unavailable (not providing annotations at all). 
    </p>
    
    <hr>
    <h4>
    Improving the Acquisition of Unlabelled Datapoints
    </h4>	
    <h5>
    How do we acquire more informative datapoints?
    </h5>  
	<p>
	In order to overcome the limitations of Monte Carlo Dropout, we propose, in our <a id='overwhite' href="https://arxiv.org/pdf/2004.09557.pdf"><u>ICML 2022 paper</u></a>, a family of consistency-based active learning frameworks. Whereas MCD perturbs neural network parameters to identify datapoints in proximity to 
	the decision boundary, we perturb the input datapoints themselves (which we refer to as Monte Carlo Perturbations) or both the input datapoints and the network parameters (which we refer to as Bayesian Active Learning by Consistency). 
	The motivation for doing so is twofold. First, the perturbation of input datapoints is more understandable than the perturbation of parameters, thus providing machine learning practitioners with increased control and interpretability over the applied perturbations.
	Second, by perturbing both input datapoints and network parameters, we make it less likely to miss informative datapoints. 
	</p>
            
	<center>
	    <img src="./images/acquisition.gif" style="width:60%"> 
	</center>	
				
    <h4>
    Improving the Annotation of Newly-Acquired Unlabelled Datapoints
    </h4>	
    <h5>
    How do we minimize the labelling burden placed on physicians?
    </h5>  
	<p>
	Whereas previous work assumes that ideal oracles exist consistently throughout the learning process, we reduce our dependence on an oracle by dynamically determining whether, for each acquired unlabelled datapoint, to request a label
	from an oracle or to provide a network-based annotation (also known as a pseudo-label). They key insight here is that we delegate what would ordinarily have been annotated by an oracle to the neural network itself (prediction network below). 
	Such a decision to delegate is performed by the oracle selection network, whose details are provided in the paper. 
	In light of this selective process, we refer to this framework as selective oracle questioning in active learning (SoQal). 
	</p>

	<center>
	    <img src="./images/soqal_pipeline.png" style="width:50%">
	</center>		

    <hr>
    <h4>
    Active Learning Scenario 1 - No Oracle
    </h4>	 
    <h5>
    How does our framework perform in the absence of an oracle?
    </h5>  
		<p>
		We begin by exploring the performance of active learning frameworks without an oracle. Below, we illustrate the validation AUC of a network that is initially exposed to 30% of the labelled training data.
		We find that a network which exploits our consistency-based framework (BALC) learns faster than, and outperforms, those which exploit the remaining acquisition functions. 
		For example, BALC<sub>KLD</sub> and BALD<sub>MCD</sub> achieve AUC around 0.69 after 20 and 40 epochs of training, respectively, reflecting a two-fold increase in learning efficiency.
		</p>
    
	<center>
	    <img src="./images/no_oracle_results.png" style="width:40%">
	</center>			
				
    <h4>
    Active Learning Scenario 2 - Noise-free Oracle
    </h4>	 
    <h5>
    How does our framework perform in the presence of an ideal oracle?
    </h5>  
		<p>
		Assuming the presence of a noise-free oracle, we now explore the effect of selective oracle questioning methods. In the table below, we present the results of these experiments across all datasets when the network
		is initially exposed to only 10% of the training data. We find that SoQal consistently outperforms several baseline methods, including S-response and epsilon-greedy across the first three datasets. 
		Such a finding suggests that SoQal is well equipped to know <i>when</i> a label should be requested from an oracle.
		</p>
    
	<center>
	    <img src="./images/oracle_results.png" style="width:40%">
	</center>
				
    <h4>
    Active Learning Scenario 3 - Noisy Oracle
    </h4>	 
    <h5>
    How does our framework perform in the presence of an oracle who somtimes provides incorrect annotations?
    </h5>  
		<p>
		Building on the findings in the previous section, we now explore the performance of our oracle questioning methods with a <i>noisy</i> oracle. 
		In the figure below, we illustrate the test AUC as a function of various types and levels of noise. 
		We also present the performance with a <i>noise-free</i> oracle (horizontal dashed lines). 
		We find that SoQal is more robust to a noisy oracle than epsilon-greedy and S-response. This is evident by the higher AUC of the former relative to the latter across different noise types and magnitudes. 
		One hypothesis for this improved performance is that SoQal, by appropriately deciding when to not request a label from a noisy oracle, avoids an incorrect instance annotation, and thus allows the network to learn well.
		</p>
        
	<center>
	    <img src="./images/noisy_oracle_results.png" style="width:70%">
	</center>
				
        	<hr>
		<h4>
    Moving Forward
    </h4>		
		<h5>
		Where do we go from here?  
		</h5>	
		<p>
		We see several worthwhile avenues to explore in the near future. SoQal assumed that the annotations provided by the oracle, when requested, are <i>consistently</i> reliable. However, an oracle (e.g., a physician) is likely to experience fatigue over time and exhibit undesired variability in annotation quality. 
		Such oracle dynamics are not accounted for by our framework yet pose exciting opportunities for the future. Moreover, our framework assumed that, at most, a <i>single</i> oracle was available throughout the learning process. 
		However, clinical settings are often characterized by the presence of multiple oracles (e.g., radiologists, cardiologists, oncologists) with different areas and levels of expertise. 
		We hope the community considers incorporating these elements into an active learning framework which would prove quite valuable given the realistic nature of such a scenario.
		</p>
				
		<h4>
		Acknowledgements
		</h4>
		<p>  
		We would also like to thank <a id='overwhite' href="https://www.youtube.com/watch?v=XuF7IcZ2Zd8"><u>Melhem Barakat</u></a> for lending us his voice. 
		</p>
				
		</br>
							
					</div>
				</section>
			</article>
    
		
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/jquery.scrollex.min.js"></script>
			<script src="../../assets/js/jquery.scrolly.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
				
				
  	</body>
	
</html>
