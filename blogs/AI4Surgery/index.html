<!DOCTYPE HTML>
<!--
	Spectral by Pixelarity
	pixelarity.com @pixelarity
	License: pixelarity.com/license
-->
<html>
	<head>
		<link rel="icon" href="">
		<title>Dani Kiyasseh | Blog - AI4Surgery </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                               tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                               });
        </script>
        <script type="text/javascript"
  			src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-97381584-1', 'auto');
		  ga('send', 'pageview');

		</script>

		
		

	</head>
  
  
	<body>

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="../../index.html">Dani Kiyasseh</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
									<a href="#menu" class="menuToggle"><span>Menu</span></a>
									<div id="menu">
										<ul>
											<li><a href="../../index.html">Home</a></li>
											<li><a href="../../index.html#aboutme">About Me</a></li>
											<li><a href="../../index.html#experience">Experience</a></li>
											<li>&mdash;</li>
											<li><a href="../../publications.html">Publications</a></li>
											<li><a href="../index.html">Blog</a></li>
											<li><a href="../../datasets.html">Datasets</a></li>											
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

	<!-- Main -->
	<article id="main">
		<section class="wrapper style5">

			<div class="inner">
			<h2>
			<center>
				Quantification of Robotic Surgeries with Vision-Based Deep Learning 
			</center>
			</h2>

			<p>
			<center>
				Sunday, May 9th, 2022
			</center>
			</p>

			<p>
			<center>
				Dani Kiyasseh
			</center>
			</p>

		<p>
		It is estimated that 200 million surgeries are performed annually and worldwide. Around 15% of these surgical procedures are performed with the assistance of a robotic device. 
                Such robotic surgeries have proven to be effective in reducing intra-operative complications (read: fewer errors during surgery), shortening the post-operative recovery time for patients
                (read: patients get better faster), and improving long-term patient outcomes. 
		</p>
                
                <h4>
                The Surgical Triad
                </h4>
                  <p>
                  During a surgical procedure, robotic or otherwise, a surgeon must often navigate critical anatomical structures, such as nerves and blood vessels, avoid harming healthy tissue, and actively avoid 
                  potential complications, all the while tending to the main task at hand. This complexity is juxtaposed with the simplicity of the overarching goal of surgery: 
                  to improve post-operative surgical and patient outcomes. Recent studies have in fact shown that intra-operative surgical activity (<i>what</i> and <i>how</i> a surgical procedure is performed) can have a 
                  direct impact on long-term patient outcomes. Knowledge of this relationship can guide the provision of feedback to surgeons in order to modulate their future behaviour. We refer to the three components 
                  of intra-operative surgical activity, post-operative patient outcomes, and surgeon feedback as the <i>surgical triad</i>. 
                  </p>
                
		<center>
		<img src="./images/surgical_triad.gif" style="width: 60%" /> 									
		</center>
                
                <h4>
                The Core Elements of Surgery
                </h4>
                  <p>
                  To better understand the relationship between intra-operative surgical activity and post-operative patient outcomes, we believe that the core elements of surgery must be first quantified in an objective,
                  reliable, and scalable manner. This involves, for example, identifying <i>what</i> and <i>how</i> activity is performed during surgery. 
                  </p>
                  
                  <p>
                  The core elements of surgery can be understood by considering a particular robotic surgery (figure below), known as a robot-assisted radical prostatectomy (RARP), in which the prostate gland is removed from a patient's body
                  due to the presence of cancerous tissue. This procedure consists of multiple <i>steps</i> over time, such as dissection (read: cutting tissue) and suturing (read: joining tissue), which reflect the <i>wha</i> of surgery. Each of these 
                  steps can be executed through a sequence of manoeuvres, or gestures, and at a different skill level depicting low- or high-quality activity. Together, these elements reflect the <i>how</i> of surgery.  
                  </p>
                  
		<center>
		<img src="./images/elements_of_surgery.gif" style="width: 60%" /> 									
		</center>
                
                <h4>
                Quantifying the Core Elements of Surgery via Roboformer
                </h4>
                  <p>
		  In our <a href=" ">research</a>, we design a deep learning framework, entitled Roboformer, that quantifies the core elements of surgery in an objective, reliable, and scalable manner.
                  This framework is <i>unified</i> in that the same network architecture can be used, without any modifications, to achieve multiple machine learning tasks (i.e., quantify multiple elements of surgery).
                  It is also <i>vision-based</i> in that is depends exclusively on videos recorded during surgery, which are straightforward to acquire from surgical robotic devices, to perform these machine learning tasks. 
                  </p>

		<center>
		<img src="./images/roboformer.gif" style="width: 60%" /> 									
		</center>
								
		<h4>
                Phase Recognition
                </h4>
		<h5>
		Can we reliably distinguish between surgical steps?  
		</h5>	
		<p>
		We trained our framework to distinguish between three distinct surgical sub-phases (read: steps): needle handling, needle driving, and needle withdrawal. These reflect periods of time during which a surgeon
		first handles a needle to drive it through some tissue before withdrawing it as part of the suturing process. Specifically, our framework was trained on data from the University of Southern California (USC)
		and deployed on unseen videos from USC and on unseen videos from a completely different medical centre (St. Antonius Hospital located in Gronau, Germany). 
		</p>
		<p>
		We show that our framework generalizes (read: performs well) to unseen surgical videos and those from unseen surgeons across distinct medical centres. While the former is evident by the strong performance 
		(AUC > 0.90) achieved when the framework was deployed on USC data, the latter is supported by the strong performance (AUC > 0.90) when it was deployed on data from St. Antonius Hospital. 
		</p>
				
		<center>
		<img src="./images/vua_phase_results.pdf" /> 									
		</center>					
								
		<h4>
                Gesture Classification
                </h4>
		<h5>
		Can we reliably distinguish between surgical gestures?  
		</h5>	
		<p>
		We also trained our framework to distinguish between surgical gestures (read: manoeuvres) commonly performed by surgeons. These reflect the decisions made by a surgeon over time about how to execute a particular 
		surgical step. Once again, our framework was trained on data from USC and deployed on unseen videos of the same surgery (RARP) from USC and St. Antonius Hospital. In this setting, we also deployed our framework
		on videos from an entirely different surgical procedure, known as a robot-assisted partial nephrectomy (RAPN), in which a portion of a patient's kidney is removed due to the presence of cancerous tissue.
		</p>
		<p>
		We show that our framework generalizes to unseen videos, surgeons, medical centres, and surgical procedures. This is evident by its strong performance (see below) across the board in all of these settings. 
		</p>
			
		<center>
		<img src="./images/gesture_classification_results.pdf" /> 									
		</center>
				
		<h4>
                Skills Assessment
                </h4>	
		<h5>
		Can we reliably distinguish between low- and high-skill activity?  
		</h5>	
		<p>
		We then trained our framework to delineate surgical activity depicting low and high technical skill. This activity ranged from how the surgeon handled a needle during the suturing step (needle handling) and how the
		surgeon drove the needle through the tissue (needle driving). For needle handling, a low-skill assessment was based on the number of times a surgeon had to reposition their grasp of the needle (more repositions = lower quality). 
		For needle driving, a low-skill assessment was based on the smoothness with which it was performed (less smooth = lower quality). As before, we trained our framework on data from USC and deployed it on data from St. Antonius Hospital. 
		</p>
		<p>
		We show that our framework, once again, generalizes to unseen videos, surgeons, and medical centres. This is evident by its strong performance (AUC > 0.80) across the skills assessment tasks and clinical settings. 
		Our framework (which is based on a Transformer architecture) also naturally lends itself to explainable findings. Below, we illustrate the relative importance of individual frames in a surgical video, as identified by the framework, and
		show that the frames with the highest level of importance do indeed align with the ground-truth low-skill assessment of the video segments.
		</p>
			
		<center>
		<img src="./images/skills_assessment_results.pdf" /> 									
		</center>
				
		<h4>
                Combining Elements of Surgery
                </h4>		
		<h5>
		How do we practically leverage the findings of this study?  
		</h5>	
		<p>
		Thus far, we have presented our machine learning tasks, which quantify the various elements of surgery, as independent of one another. 
		Considering these tasks in unison, however, suggests that our framework can provide a surgeon with reliable, objective, and scalable feedback of the following form: 
		"when completing stitch number 3 of the suturing step, your needle handling (what - <i>sub-phase</i>) was executed poorly (how - <i>skill</i>). 
		This is likely due to your activity in the first and final quarters of the needle handling sub-phase (why - <i>attention</i>). 
		Such granular and temporally-localized feedback allows a surgeon to better focus on the element of surgery that requires improvement. 
		As such, a surgeon can now better identify, and learn to avoid, problematic intra-operative surgical behaviour in the future.
		</p>					

		<h4>
		Acknowledgements
		</h4>
		<p>  
		We would also like to thank <a id='overwhite' href="https://www.youtube.com/watch?v=ZFkV6mwHhX0"><u>Wadih El Safi</u></a> for lending us his voice. 
		</p>
				
		</br>
							
					</div>
				</section>
			</article>
    
		
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/jquery.scrollex.min.js"></script>
			<script src="../../assets/js/jquery.scrolly.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
				
				
  	</body>
	
</html>
